#!/usr/bin/env python3
"""
Tests Pouss√©s du Bloc 4 - Orchestrateur LangGraph
=================================================

Tests avanc√©s pour valider toutes les fonctionnalit√©s du Bloc 4 :
- Corrections automatiques
- Gestion d'erreurs
- Retry et fallback
- Formatage des r√©ponses
"""

import sys
import os
import logging
import time
from typing import Dict, List, Any

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

# Ajouter le r√©pertoire parent au path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_bloc4_advanced():
    """Tests pouss√©s du Bloc 4"""
    print("üß™ TESTS PUS√âS BLOC 4 - ORCHESTRATEUR LANGGRAPH")
    print("=" * 60)
    
    try:
        from orchestration.langgraph_orchestrator import LangGraphOrchestrator
        
        # Initialiser l'orchestrateur
        print("üîß Initialisation de l'orchestrateur...")
        orchestrator = LangGraphOrchestrator()
        
        # Tests des diff√©rents composants
        tests = [
            test_corrections_automatiques,
            test_gestion_erreurs,
            test_retry_fallback,
            test_formatage_reponses,
            test_performance,
            test_cas_complexes
        ]
        
        results = []
        for test_func in tests:
            print(f"\nüîç {test_func.__name__.replace('_', ' ').title()}...")
            result = test_func(orchestrator)
            results.append(result)
            print(f"   {'‚úÖ' if result['success'] else '‚ùå'} {result['description']}")
        
        # R√©sum√©
        success_count = sum(1 for r in results if r['success'])
        total_count = len(results)
        
        print(f"\nüìä R√âSUM√â DES TESTS:")
        print(f"   Tests r√©ussis: {success_count}/{total_count}")
        print(f"   Taux de succ√®s: {success_count/total_count*100:.1f}%")
        
        return success_count == total_count
        
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {e}")
        return False

def test_corrections_automatiques(orchestrator) -> Dict[str, Any]:
    """Test des corrections automatiques"""
    questions_with_corrections = [
        "Quelle est ma consommation moyenne par jour ?",  # mean ‚Üí sum + post_processing
        "Combien ai-je consomm√© cette semaine ?",         # week ‚Üí 7d
        "Quelle est ma consommation totale ?",             # all ‚Üí current_year
        "Montre-moi ma consommation d'hier",              # hier ‚Üí last_day
    ]
    
    corrections_detected = 0
    
    for question in questions_with_corrections:
        try:
            response = orchestrator.process_question(question)
            corrections = response.get('corrections_applied', [])
            
            if corrections:
                corrections_detected += 1
                print(f"     ‚úÖ Corrections d√©tect√©es pour '{question}': {corrections}")
            
        except Exception as e:
            print(f"     ‚ùå Erreur pour '{question}': {e}")
    
    return {
        'success': corrections_detected >= 2,  # Au moins 2 corrections
        'description': f'Corrections automatiques ({corrections_detected}/4)',
        'details': f'Corrections d√©tect√©es: {corrections_detected}'
    }

def test_gestion_erreurs(orchestrator) -> Dict[str, Any]:
    """Test de la gestion d'erreurs"""
    error_scenarios = [
        "Question vide",                    # Gestion des questions vides
        "",                                 # Cha√Æne vide
        "Question avec caract√®res sp√©ciaux: @#$%^&*()",  # Caract√®res sp√©ciaux
        "Question tr√®s longue " + "x" * 1000,  # Question tr√®s longue
    ]
    
    errors_handled = 0
    
    for scenario in error_scenarios:
        try:
            response = orchestrator.process_question(scenario)
            
            if response.get('status') == 'error':
                errors_handled += 1
                print(f"     ‚úÖ Erreur g√©r√©e pour: {scenario[:50]}...")
            elif 'erreur' in response.get('answer', '').lower():
                errors_handled += 1
                print(f"     ‚úÖ Erreur d√©tect√©e dans la r√©ponse")
            
        except Exception as e:
            errors_handled += 1
            print(f"     ‚úÖ Exception g√©r√©e: {e}")
    
    return {
        'success': errors_handled >= 3,  # Au moins 3 erreurs g√©r√©es
        'description': f'Gestion d\'erreurs ({errors_handled}/4)',
        'details': f'Erreurs g√©r√©es: {errors_handled}'
    }

def test_retry_fallback(orchestrator) -> Dict[str, Any]:
    """Test du syst√®me de retry et fallback"""
    # Simuler des erreurs en modifiant temporairement la configuration
    original_config = orchestrator.config.RETRY_CONFIG.copy()
    
    try:
        # Configurer des retries rapides pour le test
        orchestrator.config.RETRY_CONFIG['max_retries'] = 2
        orchestrator.config.RETRY_CONFIG['retry_delay'] = 0.1
        
        # Test avec une question normale
        start_time = time.time()
        response = orchestrator.process_question("Quelle est ma consommation d'√©lectricit√© hier ?")
        execution_time = time.time() - start_time
        
        # V√©rifier que le syst√®me a g√©r√© les retries
        if execution_time > 0.5:  # Temps suffisant pour retries
            return {
                'success': True,
                'description': 'Syst√®me de retry fonctionnel',
                'details': f'Temps d\'ex√©cution: {execution_time:.2f}s'
            }
        else:
            return {
                'success': False,
                'description': 'Retry non d√©tect√©',
                'details': f'Temps trop court: {execution_time:.2f}s'
            }
            
    finally:
        # Restaurer la configuration originale
        orchestrator.config.RETRY_CONFIG = original_config

def test_formatage_reponses(orchestrator) -> Dict[str, Any]:
    """Test du formatage des r√©ponses"""
    test_questions = [
        "Quelle est ma consommation d'√©lectricit√© hier ?",
        "Combien ai-je consomm√© ce mois-ci ?",
        "Quelle est ma consommation moyenne par jour ?",
    ]
    
    formatted_responses = 0
    
    for question in test_questions:
        try:
            response = orchestrator.process_question(question)
            
            # V√©rifier le formatage
            if (response.get('answer') and 
                response.get('status') and 
                'total_execution_time' in response):
                formatted_responses += 1
                print(f"     ‚úÖ R√©ponse format√©e pour: {question[:30]}...")
            
        except Exception as e:
            print(f"     ‚ùå Erreur de formatage: {e}")
    
    return {
        'success': formatted_responses >= 2,  # Au moins 2 r√©ponses format√©es
        'description': f'Formatage des r√©ponses ({formatted_responses}/3)',
        'details': f'R√©ponses format√©es: {formatted_responses}'
    }

def test_performance(orchestrator) -> Dict[str, Any]:
    """Test de performance"""
    performance_tests = []
    
    for i in range(3):
        start_time = time.time()
        response = orchestrator.process_question("Quelle est ma consommation d'√©lectricit√© hier ?")
        execution_time = time.time() - start_time
        performance_tests.append(execution_time)
    
    avg_time = sum(performance_tests) / len(performance_tests)
    max_time = max(performance_tests)
    
    print(f"     ‚è±Ô∏è Temps moyen: {avg_time:.2f}s")
    print(f"     ‚è±Ô∏è Temps max: {max_time:.2f}s")
    
    return {
        'success': avg_time < 5.0 and max_time < 10.0,  # Crit√®res de performance
        'description': f'Performance ({avg_time:.2f}s moyen)',
        'details': f'Min: {min(performance_tests):.2f}s, Max: {max_time:.2f}s'
    }

def test_cas_complexes(orchestrator) -> Dict[str, Any]:
    """Test de cas complexes"""
    complex_questions = [
        "Compare ma consommation d'hier avec celle d'aujourd'hui",
        "Quelle est ma consommation moyenne par jour et combien √ßa co√ªte ?",
        "Montre-moi ma consommation de cette semaine et du mois dernier",
    ]
    
    complex_success = 0
    
    for question in complex_questions:
        try:
            response = orchestrator.process_question(question)
            
            if response.get('status') == 'success':
                complex_success += 1
                print(f"     ‚úÖ Cas complexe r√©ussi: {question[:40]}...")
            
        except Exception as e:
            print(f"     ‚ùå Cas complexe √©chou√©: {e}")
    
    return {
        'success': complex_success >= 1,  # Au moins 1 cas complexe r√©ussi
        'description': f'Cas complexes ({complex_success}/3)',
        'details': f'Cas complexes r√©ussis: {complex_success}'
    }

if __name__ == "__main__":
    success = test_bloc4_advanced()
    print(f"\nüéØ {'‚úÖ TOUS LES TESTS R√âUSSIS' if success else '‚ùå CERTAINS TESTS √âCHOU√âS'}")
    sys.exit(0 if success else 1)
