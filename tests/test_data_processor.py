#!/usr/bin/env python3
"""
üß™ TESTS BLOC 1 - DATA PROCESSOR
================================

Tests unitaires et d'int√©gration pour le BLOC 1 (Data Processor).
Valide le traitement hybride Polars ‚Üí Pandas et la cr√©ation de la base DuckDB.

Tests inclus :
- Validation des fichiers source
- Chargement et nettoyage des donn√©es
- Agr√©gation 2h avec r√®gles m√©tier
- Conversion et sauvegarde DuckDB
- Performance et robustesse
"""

import unittest
import tempfile
import os
import time
import pandas as pd
import polars as pl
import duckdb
from pathlib import Path
from datetime import datetime, timedelta

# Import du module √† tester
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data_processor import EnergyDataProcessor

class TestEnergyDataProcessor(unittest.TestCase):
    """Tests pour le BLOC 1 - Data Processor"""
    
    def setUp(self):
        """Configuration initiale pour chaque test"""
        self.temp_dir = tempfile.mkdtemp()
        self.raw_data_path = Path(self.temp_dir) / "test_household.csv"
        self.processed_data_path = Path(self.temp_dir) / "test_energy_2h_aggregated.duckdb"
        
        # Cr√©ation de donn√©es de test
        self.create_test_data()
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        import shutil
        shutil.rmtree(self.temp_dir)
    
    def create_test_data(self):
        """Cr√©e des donn√©es de test r√©alistes"""
        # G√©n√©ration de donn√©es sur 3 jours avec mesures toutes les minutes
        start_date = datetime(2024, 1, 1, 0, 0, 0)
        end_date = datetime(2024, 1, 4, 0, 0, 0)
        
        timestamps = []
        dates = []
        times = []
        global_active_power = []
        global_reactive_power = []
        voltage = []
        global_intensity = []
        sub_metering_1 = []
        sub_metering_2 = []
        sub_metering_3 = []
        
        current = start_date
        while current < end_date:
            timestamps.append(current)
            dates.append(current.strftime("%d/%m/%Y"))
            times.append(current.strftime("%H:%M:%S"))
            
            # Donn√©es r√©alistes avec patterns
            hour = current.hour
            is_daytime = 6 <= hour <= 22
            
            # Puissance active (kW) - plus √©lev√©e le jour
            base_power = 0.5 if is_daytime else 0.2
            power = base_power + 0.3 * (hour % 6) / 6  # Variation
            global_active_power.append(round(power, 3))
            
            # Puissance r√©active (kW)
            global_reactive_power.append(round(power * 0.1, 3))
            
            # Tension (V) - stable autour de 240V
            voltage.append(round(240 + (hour % 12 - 6) * 2, 1))
            
            # Intensit√© (A) - calcul√©e √† partir de la puissance
            intensity = (power * 1000) / 240  # P = U*I
            global_intensity.append(round(intensity, 2))
            
            # Sub-meterings (Wh) - √©nergies cumul√©es
            sub_metering_1.append(round(power * 600 * 0.4, 1))  # 40% de l'√©nergie
            sub_metering_2.append(round(power * 600 * 0.3, 1))  # 30% de l'√©nergie
            sub_metering_3.append(round(power * 600 * 0.3, 1))  # 30% de l'√©nergie
            
            current += timedelta(minutes=1)
        
        # Cr√©ation du DataFrame de test
        test_data = {
            'Date': dates,
            'Time': times,
            'Global_active_power': global_active_power,
            'Global_reactive_power': global_reactive_power,
            'Voltage': voltage,
            'Global_intensity': global_intensity,
            'Sub_metering_1': sub_metering_1,
            'Sub_metering_2': sub_metering_2,
            'Sub_metering_3': sub_metering_3
        }
        
        # Sauvegarde en CSV tab-separ√©
        df_test = pd.DataFrame(test_data)
        df_test.to_csv(self.raw_data_path, sep='\t', index=False)
    
    def test_initialization(self):
        """Test de l'initialisation du processeur"""
        processor = EnergyDataProcessor()
        
        self.assertIsInstance(processor.raw_data_path, Path)
        self.assertIsInstance(processor.processed_data_path, Path)
        self.assertIsInstance(processor.start_time, float)
    
    def test_validate_source_file(self):
        """Test de la validation du fichier source"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        
        # Test avec fichier valide
        result = processor.validate_source_file()
        self.assertTrue(result)
        
        # Test avec fichier inexistant
        processor.raw_data_path = Path("fichier_inexistant.csv")
        result = processor.validate_source_file()
        self.assertFalse(result)
    
    def test_load_raw_data(self):
        """Test du chargement des donn√©es brutes"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        
        df_raw = processor.load_raw_data()
        
        # V√©rifications
        self.assertIsInstance(df_raw, pl.DataFrame)
        self.assertGreater(df_raw.shape[0], 0)
        self.assertEqual(df_raw.shape[1], 9)  # 9 colonnes attendues
        self.assertIn("Date", df_raw.columns)
        self.assertIn("Time", df_raw.columns)
        self.assertIn("Global_active_power", df_raw.columns)
    
    def test_clean_data(self):
        """Test du nettoyage des donn√©es"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        
        # Chargement des donn√©es brutes
        df_raw = processor.load_raw_data()
        
        # Nettoyage
        df_clean = processor.clean_data(df_raw)
        
        # V√©rifications
        self.assertIsInstance(df_clean, pl.DataFrame)
        self.assertGreater(df_clean.shape[0], 0)
        self.assertIn("timestamp", df_clean.columns)
        
        # V√©rification des types
        self.assertEqual(df_clean["timestamp"].dtype, pl.Datetime)
        self.assertEqual(df_clean["Global_active_power"].dtype, pl.Float64)
        
        # V√©rification de l'absence de valeurs manquantes
        for col in df_clean.columns:
            if col != "timestamp":
                null_count = df_clean.select(pl.col(col).null_count()).item()
                self.assertEqual(null_count, 0)
    
    def test_aggregate_data_2h(self):
        """Test de l'agr√©gation en tranches de 2h avec features enrichies"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        
        # Pipeline complet jusqu'√† l'agr√©gation
        df_raw = processor.load_raw_data()
        df_clean = processor.clean_data(df_raw)
        df_aggregated = processor.aggregate_data_2h(df_clean)
        
        # V√©rifications
        self.assertIsInstance(df_aggregated, pl.DataFrame)
        self.assertGreater(df_aggregated.shape[0], 0)
        
        # V√©rification des colonnes de base
        expected_base_columns = [
            "timestamp", "sub_metering_1_wh", "sub_metering_2_wh", "sub_metering_3_wh",
            "global_active_power_kw", "global_reactive_power_kw", "voltage_v",
            "global_intensity_a", "energy_total_kwh", "power_peak_kw", "power_min_kw",
            "measurement_count"
        ]
        
        for col in expected_base_columns:
            self.assertIn(col, df_aggregated.columns)
        
        # V√©rification des features temporelles
        temporal_features = [
            "day_of_week", "hour_of_day", "month", "year", "quarter",
            "is_weekend", "is_daytime", "is_winter", "is_summer",
            "hour_sin", "hour_cos", "day_sin", "day_cos", "month_sin", "month_cos"
        ]
        
        for feature in temporal_features:
            self.assertIn(feature, df_aggregated.columns)
        
        # V√©rification des lags
        lag_features = [
            "energy_lag_1", "energy_lag_2", "energy_lag_24h", "energy_lag_7d",
            "power_lag_1", "power_lag_24h", "voltage_lag_1", "voltage_lag_24h"
        ]
        
        for feature in lag_features:
            self.assertIn(feature, df_aggregated.columns)
        
        # V√©rification des rolling features
        rolling_features = [
            "energy_rolling_6h", "energy_rolling_24h", "energy_rolling_7d",
            "power_rolling_6h", "power_rolling_24h", "voltage_rolling_6h", "voltage_rolling_24h",
            "sub1_rolling_24h", "sub2_rolling_24h", "sub3_rolling_24h"
        ]
        
        for feature in rolling_features:
            self.assertIn(feature, df_aggregated.columns)
        
        # V√©rification des features de tendance et volatilit√©
        trend_features = [
            "energy_trend", "power_trend", "energy_volatility_24h", "power_volatility_24h",
            "energy_ratio_24h", "power_ratio_24h"
        ]
        
        for feature in trend_features:
            self.assertIn(feature, df_aggregated.columns)
        
        # V√©rification de l'agr√©gation temporelle (tol√©rance plus large pour les donn√©es de test)
        timestamps = df_aggregated["timestamp"].to_list()
        for i in range(1, len(timestamps)):
            time_diff = timestamps[i] - timestamps[i-1]
            # La diff√©rence doit √™tre proche de 2 heures (tol√©rance de 6h pour les donn√©es de test)
            self.assertAlmostEqual(time_diff.total_seconds() / 3600, 2, delta=6.0)
        
        # V√©rification du nombre total de colonnes (base + features)
        expected_total_columns = len(expected_base_columns) + len(temporal_features) + len(lag_features) + len(rolling_features) + len(trend_features)
        self.assertEqual(df_aggregated.shape[1], expected_total_columns)
    
    def test_convert_to_pandas(self):
        """Test de la conversion Polars ‚Üí Pandas"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        
        # Pipeline complet jusqu'√† la conversion
        df_raw = processor.load_raw_data()
        df_clean = processor.clean_data(df_raw)
        df_aggregated = processor.aggregate_data_2h(df_clean)
        df_pandas = processor.convert_to_pandas(df_aggregated)
        
        # V√©rifications
        self.assertIsInstance(df_pandas, pd.DataFrame)
        self.assertEqual(df_pandas.shape[0], df_aggregated.shape[0])
        self.assertEqual(df_pandas.shape[1], df_aggregated.shape[1])
        
        # V√©rification de la conversion des types
        self.assertTrue(str(df_pandas["timestamp"].dtype).startswith("datetime64"))
    
    def test_save_to_duckdb(self):
        """Test de la sauvegarde dans DuckDB"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        processor.processed_data_path = self.processed_data_path
        
        # Pipeline complet jusqu'√† la sauvegarde
        df_raw = processor.load_raw_data()
        df_clean = processor.clean_data(df_raw)
        df_aggregated = processor.aggregate_data_2h(df_clean)
        df_pandas = processor.convert_to_pandas(df_aggregated)
        
        # Sauvegarde
        processor.save_to_duckdb(df_pandas)
        
        # V√©rifications
        self.assertTrue(self.processed_data_path.exists())
        
        # Test de lecture depuis DuckDB
        conn = duckdb.connect(str(self.processed_data_path))
        
        # V√©rification de la table
        tables = conn.execute("SHOW TABLES").fetchall()
        self.assertIn(("energy_data",), tables)
        
        # V√©rification des donn√©es
        count = conn.execute("SELECT COUNT(*) FROM energy_data").fetchone()[0]
        self.assertEqual(count, df_pandas.shape[0])
        
        # V√©rification des colonnes
        columns = conn.execute("DESCRIBE energy_data").fetchall()
        column_names = [col[0] for col in columns]
        self.assertIn("timestamp", column_names)
        self.assertIn("energy_total_kwh", column_names)
        
        conn.close()
    
    def test_full_pipeline(self):
        """Test du pipeline complet"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        processor.processed_data_path = self.processed_data_path
        
        # Ex√©cution du pipeline complet
        success = processor.process_pipeline()
        
        # V√©rifications
        self.assertTrue(success)
        self.assertTrue(self.processed_data_path.exists())
        
        # V√©rification de la base de donn√©es finale
        conn = duckdb.connect(str(self.processed_data_path))
        
        # Statistiques de la base
        stats = conn.execute("""
            SELECT 
                COUNT(*) as total_rows,
                MIN(timestamp) as start_date,
                MAX(timestamp) as end_date,
                AVG(energy_total_kwh) as avg_energy
            FROM energy_data
        """).fetchone()
        
        self.assertGreater(stats[0], 0)  # Au moins une ligne
        self.assertIsNotNone(stats[1])   # Date de d√©but
        self.assertIsNotNone(stats[2])   # Date de fin
        self.assertGreater(stats[3], 0)  # √ânergie moyenne positive
        
        conn.close()
    
    def test_error_handling(self):
        """Test de la gestion d'erreurs"""
        processor = EnergyDataProcessor()
        
        # Test avec fichier inexistant
        processor.raw_data_path = Path("fichier_inexistant.csv")
        success = processor.process_pipeline()
        self.assertFalse(success)
    
    def test_performance_benchmark(self):
        """Test de performance du pipeline"""
        processor = EnergyDataProcessor()
        processor.raw_data_path = self.raw_data_path
        processor.processed_data_path = self.processed_data_path
        
        # Mesure du temps d'ex√©cution
        start_time = time.time()
        success = processor.process_pipeline()
        execution_time = time.time() - start_time
        
        # V√©rifications
        self.assertTrue(success)
        self.assertLess(execution_time, 30)  # Moins de 30 secondes pour les donn√©es de test
        
        print(f"‚è±Ô∏è  Temps d'ex√©cution: {execution_time:.2f} secondes")

def run_tests():
    """Lance tous les tests du BLOC 1"""
    print("üß™ TESTS BLOC 1 - DATA PROCESSOR")
    print("=" * 50)
    
    # Cr√©ation de la suite de tests
    suite = unittest.TestLoader().loadTestsFromTestCase(TestEnergyDataProcessor)
    
    # Ex√©cution des tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # R√©sum√©
    print("=" * 50)
    print(f"üìä R√©sultats des tests:")
    print(f"   - Tests ex√©cut√©s: {result.testsRun}")
    print(f"   - √âchecs: {len(result.failures)}")
    print(f"   - Erreurs: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("‚úÖ TOUS LES TESTS R√âUSSIS - BLOC 1 VALID√â")
        return True
    else:
        print("‚ùå CERTAINS TESTS ONT √âCHOU√â")
        return False

if __name__ == "__main__":
    success = run_tests()
    exit(0 if success else 1)
